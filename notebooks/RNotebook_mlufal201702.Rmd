---
title: "Machine Learning Project - UFAL 2017.2"
output: html_notebook
---

O Objetivo geral do estudo é gerar um modelo de predição capaz de detectar bugs em alterações de códigos na linguagem C. 

O Dataset contém metricas de software coletadas via [API Understand] (https://scitools.com/feature/metrics/) de Funções sem bugs de projetos open source presentes no Github escrito na linguagem C.

####Projetos Utilizados:
- [Glibc](https://github.com/lattera/glibc) - The GNU C Library is the standard system C library for all GNU systems
- [Httpd](https://github.com/apache/httpd) - Web Server
- [Kernel](https://github.com/torvalds/linux) - Kernel Linux
- [OpenVPN](https://github.com/mozilla/openvpn) - 
- [Xen](https://github.com/xen-project/xen) - Virtual Machine Monitor (VMM)

Descrição do Dataset:


```{r, echo=FALSE}
input_sample <- read.csv(file = "/home/r4ph/R/machine-learning-ufal/datasets/vulnerability/balanced/glibc_data_balanced.csv", stringsAsFactors = FALSE)
head(input_sample)

```


Importando Packages de Machine Learning:

```{r}
library(RWeka)
library(e1071)
library(gmodels)
library(C50)
library(caret)
library(irr)
library(randomForest)
library(mlr)
library(evaluate)
library(FSelector)
```

Funções para Calcular a [Efetividade](https://en.wikipedia.org/wiki/Precision_and_recall) do Modelo Baseado em [K-fold Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))

  
```{r}

# Function to calculate precision
precision <- function(tp, fp) {
  precision <- tp / (tp + fp)
  
  return(precision)
}

# Function to calculate recall
recall <- function(tp, fn) {
  recall <- tp / (tp + fn)
  
  return(recall)
}

# Function to calculate F-measure
f_measure <- function(tp, fp, fn) {
  f_measure <-
    (2 * precision(tp, fp) * recall(tp, fn)) / (recall(tp, fn) + precision(tp, fp))
  
  return(f_measure)
}

# Function to calculate true_positive, true_negative, false_positive, false_negative
measures <- function(test, pred) {
  true_positive <- 0
  true_negative <- 0
  false_positive <- 0
  false_negative <- 0
  
  for (i in 1:length(pred)) {
    if (test[i] == 'VULNERABLE' && pred[i] == 'VULNERABLE') {
      true_positive <- true_positive + 1
    } else if (test[i] == 'NEUTRAL' && pred[i] == 'NEUTRAL') {
      true_negative <- true_negative + 1
    } else if (test[i] == 'NEUTRAL' && pred[i] == 'VULNERABLE') {
      false_negative <- false_negative + 1
    } else if (test[i] == 'VULNERABLE' && pred[i] == 'NEUTRAL') {
      false_positive <- false_positive + 1
    }
  }
  
  measures <-
    c(
      precision(true_positive, false_positive),
      recall(true_positive, false_negative),
      f_measure(true_positive, false_positive, false_negative)
    )
  
  return(measures)
}
```

###Algoritmos e Técnicas de Machine Learning Utilizadas no Projeto:
- J48[Arvore de Decisão]
- NaiveBayes 
- SVM
- OneR [Regras]
- RandomForest [Arvores de Decisão]
- C50 [Arvore de Decisão]

```{r}

# Techiniques
executeJ48 <- function(dataset, folds) {
  results <- lapply(folds, function(x) {
    train <- dataset[-x,]
    test <- dataset[x,]
    model <- J48(train$Affected ~ ., data = train)
    pred <- predict(model, test)
    results <- measures(test$Affected, pred)
    
    return(results)
  })
  
}

executeNaiveBayes <- function(dataset, folds) {
  results <- lapply(folds, function(x) {
    train <- dataset[-x,]
    test <- dataset[x,]
    model <- naiveBayes(train, train$Affected, laplace = 1)
    pred <- predict(model, test)
    
    results <- measures(test$Affected, pred)
    
    return(results)
  })
  
}

executeSVM <- function(dataset, folds) {
  results <- lapply(folds, function(x) {
    train <- dataset[-x, ]
    test <- dataset[x, ]
    model <- svm(train$Affected ~ ., data = train)
    pred <- predict(model, test)
    
    results <- measures(test$Affected, pred)
    
    return(results)
  })
  
}

executeOneR <- function(dataset, folds) {
  results <- lapply(folds, function(x) {
    train <- dataset[-x, ]
    test <- dataset[x, ]
    model <- OneR(train$Affected ~ ., data = train)
    pred <- predict(model, test)
    
    results <- measures(test$Affected, pred)
    
    return(results)
  })
  
}


executeRandomForest <- function(dataset, folds) {
  results <- lapply(folds, function(x) {
    train <- dataset[-x, ]
    test <- dataset[x, ]
    model <- randomForest(train$Affected ~ ., data = train)
    pred <- predict(model, test)
    
    results <- measures(test$Affected, pred)
    
    return(results)
  })
}


executeC50 <- function(dataset, folds) {
  results <- lapply(folds, function(x) {
    train <- dataset[-x, ]
    test <- dataset[x, ]
    model <- C5.0(train$Affected ~ ., data = train)
    pred <- predict(model, test)
    results <- measures(test$Affected, pred)
    return(results)
  })
  
}
```

###Feature Selection:

```{r}
features <- makeClassifTask(id = deparse(substitute(input_sample)), input_sample, colnames(input_sample)[28] )
featuresSelection = generateFilterValuesData(features, method = c("information.gain", "chi.squared"))
featuresSelection$data
```

Criando boxplots para cada tipo metodo de seleção:
```{r}
par(mfrow=c(1,2))
for(i in 3:4) {
	boxplot(featuresSelection$data[i], main=names(featuresSelection$data)[i])
}
```

Valores Individuais de Cada Feature:
```{r}
plotFilterValues(featuresSelection)
```

Manter Todas as Features com um grau/valor de importância de 0.2


```{r}
fv = generateFilterValuesData(features, method = "information.gain")
filtered = filterFeatures(features, fval = fv, threshold = 0.2)
filtered
```


###Balanceamento dos Dados:

```{r}
input_balanced <- read.csv(file = "/home/r4ph/R/machine-learning-ufal/datasets/vulnerability/balanced/glibc_data_balanced.csv", stringsAsFactors = FALSE)
input_unbalanced <- read.csv(file = "/home/r4ph/R/machine-learning-ufal/datasets/vulnerability/unbalanced/glibc_data.csv", stringsAsFactors = FALSE)
# Criando Histogramas
par(mfrow=c(1,2))
barplot(table(input_balanced$Affected), main = "Balanced")
barplot(table(input_unbalanced$Affected), main = "Unbalanced")


```


###Resultados Alcançados:

```{r}
resultsUnbal <- read.csv(file = "/home/r4ph/R/machine-learning-ufal/datasets/vulnerability/results/ml_balanced.csv", stringsAsFactors = FALSE)
resultsUnbal
```

###Plots:

```{r}
ggplot(resultsUnbal) +
  geom_bar(aes(x = Project, y = F.Measure, fill = Algorithm, group = Algorithm), position = "dodge", stat = "identity") +
  geom_text( aes(x = Project, y = F.Measure, label = round(F.Measure,2), group = Algorithm),  
             check_overlap = TRUE, position = position_dodge(width = 1), vjust = -0.5, size = 2) + #scale_fill_grey() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(legend.position= c(0.85, 0.95), legend.direction="horizontal", legend.title = element_blank(), 
        axis.title.x=element_blank(), legend.text=element_text(size=5), legend.background = element_rect(fill = "transparent", colour = NA) ) +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))
```

